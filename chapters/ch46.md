> # 46. Reinforcement learning example

# 46. Ví dụ về Học tăng cường

![img](../imgs/C46_01.png)

> Suppose you are using machine learning to teach a helicopter to fly complex maneuvers. Here is a time-lapse photo of a computer-controller helicopter executing a landing with the engine turned off.

Giả sử như bạn đang sử dụng học máy để dạy trực thăng bay theo những chuyển động phức tạp. Đây là một tấm ảnh time-lapse của một chiếc trực thăng được máy tính điều khiển đang hạ cánh khi động cơ đã tắt.

> This is called an "autorotation" maneuver. It allows helicopters to land even if their engine unexpectedly fails. Human pilots practice this maneuver as part of their training. Your goal is to use a learning algorithm to fly the helicopter through a trajectory ​*T* t​hat ends in a safe landing.

Đây được gọi là kĩ thuật "quay tự động". Nó cho phép trực thăng hạ cánh ngay cả khi động cơ bị ngắt ngoài dự kiến. Kĩ thuật bay này là một phần trong việc huấn luyện của người phi công. Nhiệm vụ của bạn là sử dụng một thuật toán học tập để lái chiếc trực thăng qua một quỹ đạo *T* và kết thúc với một pha hạ cánh an toàn.

> To apply reinforcement learning, you have to develop a "Reward function" ​*R​*(.) that gives a score measuring how good each possible trajectory ​*T​* is. For example, if ​*T* ​results in the helicopter crashing, then perhaps the reward is ​*R(T)​* = -1,000—a huge negative reward. A trajectory ​*T​* resulting in a safe landing might result in a positive ​*R(T)* w​ ith the exact value depending on how smooth the landing was. The reward function ​*R*(​.) is typically chosen by hand to quantify how desirable different trajectories ​*T​* are. It has to trade off how bumpy the landing was, whether the helicopter landed in exactly the desired spot, how rough the ride down was for passengers, and so on. It is not easy to design good reward functions.

Để áp dụng học tăng cường, bạn phải phát triển một "hàm điểm thưởng" *R*(.) nó sẽ cho một điểm số để đo xem mỗi quỹ đạo *T* tốt như thế nào. Lấy ví dụ, nếu *T* kết thúc bằng việc trực thăng bị rơi, thì điểm thưởng có thể sẽ là *R(T)* = -1.000-một điểm thưởng âm rất lớn. Một quỹ đạo *T* kết thúc bằng việc trực thăng hạ cánh an toàn có thể sẽ cho *R(T)* dương với giá trị chính xác phụ thuộc vào việc hạ cánh êm ái như thế nào. Hàm điểm thưởng *R*(.) thường được chọn thủ công để định lượng mức độ mong muốn của những quỹ đạo *T* khác nhau. Nó phải đánh đổi giữa những đặc tính như mức độ xóc của cú hạ cánh, trực thăng có hạ cánh đúng vị trí mong muốn không, quá trình hạ độ cao có nhiều biến động đối với hành khách không, và vân vân. Thiết kế một hàm điểm thưởng tốt không hề dễ dàng.

> Given a reward function ​*R(T)*, ​the job of the reinforcement learning algorithm is to control the helicopter so that it achieves max​<sub>*T*</sub>​*R(T)*. ​However, reinforcement learning algorithms make many approximations and may not succeed in achieving this maximization.

Cho một hàm điểm thưởng *R(T)*, công việc của thuật toán học tăng cường là để điều khiển trực thăng sao cho nó đạt được điểm thưởng cao nhất max<sub>*T*</sub>*R(T)*. Tuy nhiên, thuật toán học tăng cường có nhiều phép ước lượng và có thể sẽ không thành công trong việc tối ưu này.

> Suppose you have picked some reward ​*R(.)​* and have run your learning algorithm. However,
> its performance appears far worse than your human pilot—the landings are bumpier and
> seem less safe than what a human pilot achieves. How can you tell if the fault is with the
> reinforcement learning algorithm—which is trying to carry out a trajectory that achieves
> max​<sub>*T*</sub>​*R(T)*—​ or if the fault is with the reward function—which is trying to measure as well as specify the ideal tradeoff between ride bumpiness and accuracy of landing spot?

Giả sử bạn đã có vài điểm thưởng *R(.)* và đã chạy thuật toán học của bạn. Tuy nhiên chất lượng của nó còn thua xa chất lượng của con người-hạ cánh xóc hơn và trông kém an toàn hơn so với chất lượng của phi công. Làm sao để bạn biết được liệu đó có phải là lỗi của thuật toán học tăng cường-cái mà đang đang cố thực thi một quỹ đạo để tối đa max<sub>*T*</sub>*R(T)*- hay là lỗi của hàm điểm thưởng-cái mà đang cố đo cũng như xác định mức đánh đổi tối ưu giữa độ xóc của chuyến bay và độ chính xác của vị trí hạ cánh. 

> To apply the Optimization Verification test, let *T*<sub>human</sub>​ be the trajectory achieved by the human pilot, and let *T*<sub>out</sub>​ be the trajectory achieved by the algorithm. According to our description above, *T*<sub>human</sub>​ is a superior trajectory to *T*<sub>out</sub>​. Thus, the key test is the following:
> Does it hold true that *R*(*T*<sub>human</sub>) > *R*(*T*<sub>out</sub>​)?

Để áp dụng bài kiểm tra xác minh tố ưu, cho *T*<sub>người</sub> là quỹ đạo bay của phi công con người, và cho *T*<sub>ra</sub> là quỹ đạo đạt được của thuật toán. Dựa theo mô tả ở phía trên của chúng ta, *T*<sub>người</sub> là quỹ đạo tốt hơn so với *T*<sub>ra</sub>. Do vậy, bài kiểm tra chính là:
Liệu có đúng không khi *R*(*T*<sub>người</sub>) > *R*(*T*<sub>ra</sub>)?

> Case 1: If this inequality holds, then the reward function ​*R(​.)* is correctly rating *T*<sub>human</sub>​ as superior to *T*<sub>out</sub>​. But our reinforcement learning algorithm is finding the inferior *T*<sub>out</sub>​. This suggests that working on improving our reinforcement learning algorithm is worthwhile.

Trường hợp 1: Nếu bất đẳng thức này đúng, thì hàm điểm thưởng *R(.)* đang đánh giá đúng rằng *T*<sub>người</sub> vượt trội hơn so với *T*<sub>ra</sub>. Nhưng vậy thì thuật toán học tăng cường của chúng ta đang tìm *T*<sub>ra</sub> kém hơn. Điều này gợi ý rằng bỏ công sức cải thiện thuật toán học tăng cường của chúng ta là xứng đáng. 

> Case 2: The inequality does not hold: *R*(*T*<sub>human</sub>) ≤ *R*(*T*<sub>out</sub>​). This mean *R​(.)* assigns a worse score to *T*<sub>human</sub>​ even though it is the superior trajectory. You should work on improving ​*R​(.)* to better capture the tradeoffs that correspond to a good landing.

Trường hợp 2: Bất đẳng thức trên không đúng: *R*(*T*<sub>người</sub>) ≤ *R*(*T*<sub>ra</sub>​). Tức là *R(.)* đang gán cho *T*<sub>người</sub> một điểm số tệ hơn dù cho nó là quỹ đạo tốt hơn. Bạn nên cải thiện *R(.)* để có thể nắm bắt việc đánh đổi giữa các tiêu chí tương đương với một cú hạ cánh tốt.

> Many machine learning applications have this "pattern" of optimizing an approximate
> scoring function Score​<sub>x</sub>​(.) using an approximate search algorithm. Sometimes, there is no specified input ​*x*,​ so this reduces to just Score(.). In our example above, the scoring function was the reward function Score(​*T*)​ = R(​*T*)​ , and the optimization algorithm was the reinforcement learning algorithm trying to execute a good trajectory ​*T*.​

Nhiều ứng dụng machine learning có chung "khuôn mẫu" là tối ưu xấp xỉ một hàm tính điểm Điểm<sub>x</sub>(.) sử dụng một thuật toán tìm kiếm xấp xỉ. Đôi khi cũng không tồn tại một đầu vào *x* được chỉ định trước, vậy nên nó suy giảm thành Điểm(.). Trong ví dụ trên của chúng ta, hàm tính điểm chính là hàm điểm thưởng Điểm(*T*) = R(*T*), và thuật toán tối ưu là thuật toán học tăng cường đang cố thực thi một quỹ đạo bay *T* tốt.

> One difference between this and earlier examples is that, rather than comparing to an "optimal" output, you were instead comparing to human-level performance *T*<sub>human</sub>​. We assumed *T*<sub>human</sub>​ is pretty good, even if not optimal. In general, so long as you have some y* (in this example, *T*<sub>human</sub>​) that is a superior output to the performance of your current learning algorithm—even if it is not the "optimal" output—then the Optimization Verification test can indicate whether it is more promising to improve the optimization algorithm or the scoring function.

Một điểm khác biệt so với những ví dụ trước là, thay vì so sánh với một kết quả "tối ưu", bạn so sánh với chất lượng mức con người *T*<sub>người</sub>. Chúng ta giả sử *T*<sub>người</sub> khá là tốt, hoặc thậm chí là tối ưu. Nhìn chung, miễn là bạn có kết quả y* (trong ví dụ này, *T*<sub>người</sub>) tốt hơn so với thuật toán học của bạn hiện thời-mặc dù có thể nó không phải là kết quả "tối ưu"-thì Bài kiểm tra xác minh tối ưu có thể chỉ ra xem liệu cải thiện thuật toán tối ưu hay cải thiện hàm tính điểm sẽ hứa hẹn hơn. 
